%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 2.0 (10/06/16)
%
\documentclass[aspectratio=169]{beamer}
\beamertemplatenavigationsymbolsempty%
\usepackage{textpos}
\usefonttheme[onlymath]{serif}
\usepackage{amsmath}
\usepackage{empheq}
\usepackage{fontspec}
\usepackage{hyperref}
\usepackage{textcomp}
\usepackage{multirow}
\usepackage[document]{ragged2e}
\setsansfont{Linux Biolinum O}
%\setmainfont{Gentium}
\usetheme{Rochester}% theme
\usecolortheme{seahorse}
\colorlet{beamer@blendedblue}{green!40!black}
%\setbeamertemplate{navigation symbols}{}
%\setbeamertemplate{footline}[frame number]
\setbeamertemplate{footline}{% hide total number of slides
  \hfill% 
  \usebeamercolor[fg]{page number in head/foot}% 
  \usebeamerfont{page number in head/foot}% 
  %\insertframenumber%
  \insertpagenumber
  \kern1em\vskip2pt% 
}
\setbeamertemplate{itemize items}[circle]
\hypersetup{
    colorlinks = true,
    urlcolor = cyan
}

% Define color "lightgreen" for highlighting equarions
\definecolor{lightgreen}{HTML}{90EE90}

\addtobeamertemplate{frametitle}{}{%
\begin{textblock*}{100mm}(.93\textwidth,-0.77cm)
\includegraphics[height=0.77cm]{figures/csu-logo.png}
\end{textblock*}
\begin{textblock*}{100mm}(.995\textwidth,-1.6cm)
\includegraphics[height=0.8cm]{figures/Lang_candidate_for_NOvA-logo-2.eps}
\end{textblock*}}

\title{Iterative Unfolding Optimization with the Mean Squared Error Metric}
\date[today]{ND Working Group\\ NOvA Collaboration Meeting @ Fermilab\\ \today}
\author{Shih-Kai Lin}
\institute{Colorado State University}
\usepackage{multicol} % 
%\usepackage{animate}  % animation
\usepackage{amsmath,amsfonts,amssymb} % This makes the equations appears better 

\titlegraphic{\includegraphics[height=1cm]{figures/Lang_candidate_for_NOvA-logo-2.eps}\hspace*{6cm}~%
   \includegraphics[height=1cm]{figures/csu-logo.png}
}

\begin{document}
%The title
\begin{frame}
\titlepage
\end{frame}

%================================================================================
\begin{frame}{Recap}
  \begin{itemize}
    \item The standard metric in the ND group used by all analyses requiring unfolding is the average global correlation coefficient\footnote[frame]{Stefan Schmitt, ``Data Unfolding Methods in High Energy Physics''},
      \begin{equation} \label{eq:1}
        \rho_{avg}=\frac{1}{M}\sum_{j=1}^{M}\sqrt{1-\frac{1}{\pmb{V}_{jj}(\pmb{V}^{-1})_{jj}}}
      \end{equation}
    , where $M$ is the number of bins and $\pmb{V}$ is the covariance matrix in true space inferred by the unfolding algorithm.
    \item For analyses with tens of bins, this is a convenient metric. However, for an analysis with thousands of bins, this metric turned out to be infeasible.
  \end{itemize}
\end{frame}

%================================================================================
\begin{frame}{Meaning of Average Global Correlation Coefficient}
  \begin{columns}
    \begin{column}{.4\textwidth}
      \centering
      \includegraphics[height=\textheight]{figures/james_book.jpg}
    \end{column}
    \begin{column}{.6\textwidth}
      \small
      From the book by the authors of MINUIT, p.27:\\
      \noindent\fbox{%
        \parbox{\textwidth}{%
      ``Consider the correlation $\rho(X_k,Y)$ between the variable $X_k$ and every possible linear combination $Y$ of all the other variables $X_i$, $i\neq k$. A useful quantity is the \textit{global correlation coefficient} $\rho_k$, defined as the largest value of $\rho(X_k,Y)$. This quantity is a measure of the total amount of correlation between $X_k$ and all the other variables.''
        }%
      }\\
      \vspace{\baselineskip}
      In unfolding's context, it says ``I want the bin-averaged maximum possible bin-to-bin correlation to be as small as possible.''\\
      This is intuitively what ``unsmearing" means.
    \end{column}
  \end{columns}
\end{frame}

%================================================================================
\begin{frame}{Infeasibility of Average Global Correlation Coefficient for Many-Bin Analyses}
  \begin{columns}
    \begin{column}{.65\textwidth}
    Inverting a covariance matrix this large turns out to be very tricky.
    \begin{itemize}
      \small
      \item The covariance matrices all have astronomical \textcolor{red}{condition numbers} (i.e., ill-conditioned or nearly singular).
      \item Numerical inversion is still possible but subject to an arbitrary, small cut-off number, or tolerance, brought into play by SVD.
      \item Forcefully getting the calculation through results in numerical instability, such as negative values in square root in Eq.~\ref{eq:1}. Removing unphysical values, results are shown to the right. No clear minimum is observed.
      \item Last week I learned from Travis that even though the matrix is well-conditioned, a minimum is not guaranteed.
    \end{itemize}
    \end{column}
    \begin{column}{.35\textwidth}
      \includegraphics[width=\textwidth]{figures/cov_mat_iter20.png} \\
      \includegraphics[width=\textwidth]{figures/avg_rho.eps}
    \end{column}
  \end{columns}
\end{frame}

%================================================================================
\begin{frame}{A Simple Alternative Metric: Mean Squared Error}
  \begin{columns}
    \begin{column}{.5\textwidth}
      \footnotesize
			Suppose $\theta$ is the a true parameter to be estimated, and $\hat{\theta}$ is an estimator of the parameter.
			The mean squared error (MSE) can be decomposed into a sum of variance and bias squared.
			\begin{eqnarray} \label{eq:2}
			MSE &=& E[(\hat{\theta}-\theta)^2]\nonumber\\
			&=& E[\hat{\theta}^2-2\hat{\theta}\theta+\theta^2] = E[\hat{\theta}^2] -2\theta E[\hat{\theta}]+\theta^2 \nonumber\\
			&=& (E[\hat{\theta}^2]-E[\hat{\theta}]^2)+(E[\hat{\theta}]^2-2\theta E[\hat{\theta}]+\theta^2) \nonumber\\
			&=& V[\hat{\theta}]+b^2
			\end{eqnarray}
			, where $b=E[\hat{\theta}]-\theta$ is the bias of the estimator.\\
			Equation~\ref{eq:2} is called bias-variance decomposition.
			\vfill
			This is a very common metric in statistics and machine learning as well.
	\end{column}
    \begin{column}{.5\textwidth}
      \includegraphics[width=\textwidth]{figures/mse_illustration_iter20.pdf}
    \end{column}
  \end{columns}
\end{frame}

%================================================================================
\begin{frame}{Mean Squared Error in the Context of Unfolding}
  \begin{columns}
    \begin{column}{.5\textwidth}
      \footnotesize
				Given a true histogram $\pmb{\mu}=(\mu_1,...,\mu_M)$, where $\mu_i$'s are true counts in the $i$-th bin, $i=1,...,M$, unfolding can be viewed as a procedure that outputs a vector of estimators for the bin counts $\pmb{\hat{\mu}}=(\hat{\mu}_1,...,\hat{\mu}_M)$.
				
				Denoting the mean squared error for the $i$-th bin $MSE_i$, MSE for the histogram can be defined as
				\begin{eqnarray}
				  MSE &=& \frac{1}{M}\sum_{i=1}^M MSE_i \nonumber\\
				  &=& \frac{1}{M}\sum_{i=1}^M V_{ii}+\hat{b}_i^2
				\end{eqnarray}
				, where $V_{ii}=cov[\hat{\mu}_i,\hat{\mu}_i]$ and $\hat{b}_i$ is an estimator of $E[\hat{\mu}_i]-\mu_i$.
    \end{column}
    \begin{column}{.5\textwidth}
      \footnotesize
			Very often one wants to estimate more accurately bins with smaller absolute statistical uncertainties. In this case, weighted MSE can be used:
			\begin{eqnarray}
			  weighted \quad MSE &=& \frac{1}{M}\sum_{i=1}^M \frac{MSE_i}{\hat{\mu}_i} \nonumber\\
			  &=& \frac{1}{M}\sum_{i=1}^M \frac{V_{ii}+\hat{b}_i^2}{\hat{\mu}_i}
			\end{eqnarray}
			In this study, a minimum bin count is required to satisfy Poisson statistics, and is compared to the result without count constraint.
		\end{column}
	\end{columns}
\end{frame}

%================================================================================
\begin{frame}{Accessing MSE with Toy Monte Carlo}  
  If I had 100 statistically independent reco MC spectra, I could have started with step 3. Since I don't, I do:
  \begin{enumerate}
    \small
    \item Take the true spectrum. Generate 100 pseudo experiments by fluctuating each bin by Poisson with the true bin count as the parameter, i.e., average value.
    \item Smear each experiment by applying the normalized migration matrix to the true spectrum.
      \begin{equation}
        \nu_i=\sum_j\left(\frac{A_{ij}}{\sum_i A_{ij}}\right)\mu_j
      \end{equation}
      , where $\nu_i$ is the reco spectrum, $\mu_j$ is the true spectrum, and $A_{ij}$ is the migration matrix from \texttt{CAFAna}.
    \item Unfold each spectrum by a certain number of iterations.
    \item For each iteration, calculate MSE with the 100 spectra. Find the number of iterations with a minimum MSE.
  \end{enumerate}
\end{frame}

%================================================================================
\begin{frame}{Why My First Attempt Did Not Work Out}
  \begin{columns}
    \begin{column}{.5\textwidth}
    \begin{itemize}
      \footnotesize
      \item My first try led to exceedingly large bias, washing out the variance term completely.
      \item All ND analyzers have been using the ``migration matrix'' obtained by \texttt{CAFAna} directly. My first attempt used the ``normalized'' migration matrix instead, resulting in large unfolded counts at bins with almost no counts in true space.
        \begin{equation*}
          B_{ij}=\frac{A_{ij}}{\sum_i A_{ij}}
        \end{equation*}
      \item I used the raw ``migration matrix'' in my second attempt and got the same results as others. Probably double counting somewhere.
    \end{itemize}
    \end{column}
    \begin{column}{.5\textwidth}
      \includegraphics[width=\textwidth]{figures/unfolded_old_new_comparison.eps}
    \end{column}
  \end{columns}
\end{frame}

%================================================================================
\begin{frame}{Pseudo-experiments Generated}
  \begin{columns}
    \begin{column}{.5\textwidth}
      \includegraphics[width=\textwidth]{figures/pseudoexp_true.eps}
    \end{column}
    \begin{column}{.5\textwidth}
      \includegraphics[width=\textwidth]{figures/pseudoexp_reco.eps}
    \end{column}
  \end{columns}
\end{frame}

%================================================================================
\begin{frame}{Unfolded Spectra (Input, Standard Deviation as Error)}
  \centering
  \includegraphics[height=\textheight]{figures/unfolded_pseudo_exps_iter0.eps}
\end{frame}

%================================================================================
\begin{frame}{Unfolded Spectra 1 Iteration (Standard Deviation as Error)}
  \centering
  \includegraphics[height=\textheight]{figures/unfolded_pseudo_exps_iter1.eps}
\end{frame}

%================================================================================
\begin{frame}{Unfolded Spectra 2 Iteration (Standard Deviation as Error)}
  \centering
  \includegraphics[height=\textheight]{figures/unfolded_pseudo_exps_iter2.eps}
\end{frame}

%================================================================================
\begin{frame}{Unfolded Spectra 4 Iteration (Standard Deviation as Error)}
  \centering
  \includegraphics[height=\textheight]{figures/unfolded_pseudo_exps_iter4.eps}
\end{frame}

%================================================================================
\begin{frame}{Unfolded Spectra 6 Iteration (Standard Deviation as Error)}
  \centering
  \includegraphics[height=\textheight]{figures/unfolded_pseudo_exps_iter6.eps}
\end{frame}

%================================================================================
\begin{frame}{Unfolded Spectra 10 Iteration (Standard Deviation as Error)}
  \centering
  \includegraphics[height=\textheight]{figures/unfolded_pseudo_exps_iter10.eps}
\end{frame}

%================================================================================
\begin{frame}{Unfolded Spectra 100 Iteration (Standard Deviation as Error)}
  \centering
  \includegraphics[height=\textheight]{figures/unfolded_pseudo_exps_iter100.eps}
\end{frame}

%================================================================================
\begin{frame}{Observations}
  \begin{itemize}
    \item Iterative unfolding converges very quickly for all analyses we have seen so far. Hundreds of iterations might be out of the question.
    \item The central values of pseudo-experiments move towards the true values with the number of iterations at the expense of the spreads.
    \item A fun fact: \texttt{RooUnfold}'s iterative unfolding strictly preserves the total number of events for a given input, regardless of the iterations. This is not true for some other algorithms, e.g. \texttt{TUnfold}.
  \end{itemize}
\end{frame}

%================================================================================
\begin{frame}{Results: Unweighted MSE}
  \begin{columns}
    \begin{column}{.5\textwidth}
      \centering
      Full Available Range
      \includegraphics[width=\textwidth]{figures/mse_vs_iter.pdf}
    \end{column}
    \begin{column}{.5\textwidth}
      \centering
      Zoom In
      \includegraphics[width=\textwidth]{figures/mse_vs_iter_iterlim20_grid.pdf}
    \end{column}
  \end{columns}
\end{frame}

%================================================================================
\begin{frame}{Results: Weighted MSE}
  \begin{columns}
    \begin{column}{.5\textwidth}
      \includegraphics[width=\textwidth]{figures/mse_vs_iter_true_weighted_mincnt_0.pdf}
    \end{column}
    \begin{column}{.5\textwidth}
      \includegraphics[width=\textwidth]{figures/mse_vs_iter_true_weighted_mincnt_100.pdf}
    \end{column}
  \end{columns}
\end{frame}

%================================================================================
\begin{frame}{Reference}
  \begin{columns}
    \begin{column}{.5\textwidth}
      Most of the contents in this document are taken from this textbook, especially Chapter 11 dedicated to unfolding.
    \end{column}
    \begin{column}{.5\textwidth}
      \centering
      \includegraphics[height=\textheight]{figures/cowan_front_cover.jpeg}
    \end{column}
  \end{columns}
\end{frame}

%================================================================================
\begin{frame}{Conclusion}
Below I only talk about unweighted MSE.
  \begin{itemize}
    \item If you have a well-conditioned covariance matrix, the average global correlation coefficient might be the metric to go, since \texttt{RooUnfold} gives you the covariance matrix along the way.
    \item Mean squared error is a generic and conceptually simple metric. It is just much more work.
    \item For $\nu_\mu$ CC inclusive analysis, the curve is very flat around the minimum. Moving slightly away from the minimum of this study should have a negligible effect.\\
    Crude estimate: (score(3)$^{1/2}$-score(6)$^{1/2}$)/average\_bin\_count \\
    $(\sqrt{266.87}-\sqrt{261.33})/314.29=0.05\%$ difference in relative error!
  \end{itemize}
\end{frame}

%\begin{frame}{Mean Squared Error \& \\Bias-Variance Decomposition}
%Suppose $\theta$ is the a true parameter to be estimated, and $\hat{\theta}$ is an estimator of the parameter.
%The mean squared error (MSE) can be decomposed into a sum of variance and bias squared.
%\begin{eqnarray}
%MSE &=& E[(\hat{\theta}-\theta)^2]\nonumber\\
%&=& E[\hat{\theta}^2-2\hat{\theta}\theta+\theta^2] = E[\hat{\theta}^2] -2\theta E[\hat{\theta}]+\theta^2 \nonumber\\
%&=& (E[\hat{\theta}^2]-E[\hat{\theta}]^2)+(E[\hat{\theta}]^2-2\theta E[\hat{\theta}]+\theta^2) \nonumber\\
%&=& V[\hat{\theta}]+b^2
%\end{eqnarray}
%, where $b=E[\hat{\theta}]-\theta$ is the bias of the estimator.
%\vfill
%This is a very common metric in machine learning as well.
%\end{frame}
%
%\begin{frame}{Mean Squared Error in the Context of Unfolding}
%Given a true histogram $\pmb{\mu}=(\mu_1,...,\mu_M)$, where $\mu_i$'s are true counts in the $i$-th bin, $i=1,...,M$, unfolding can be viewed as a procedure that outputs a vector of estimators for the bin counts $\pmb{\hat{\mu}}=(\hat{\mu}_1,...,\hat{\mu}_M)$.
%
%Denoting the mean squared error for the $i$-th bin $MSE_i$, MSE for the histogram can be defined as
%\begin{eqnarray}
%  MSE &=& \frac{1}{M}\sum_{i=1}^M MSE_i \nonumber\\
%  &=& \frac{1}{M}\sum_{i=1}^M U_{ii}+\hat{b}_i^2
%\end{eqnarray}
%, where $U_{ii}=cov[\hat{\mu}_i,\hat{\mu}_i]$ and $\hat{b}_i$ is an estimator of $E[\hat{\mu}_i]-\mu_i$.
%\end{frame}
%
%%=================================================================
%\begin{frame}{Mean Squared Error in the Context of Unfolding (Cont.)}
%Very often one wants to estimate more accurately bins with smaller statistical uncertainties. In this case, weighted MSE can be used:
%\begin{eqnarray}
%  weighted \quad MSE &=& \frac{1}{M}\sum_{i=1}^M \frac{MSE_i}{\hat{\mu}_i} \nonumber\\
%  &=& \frac{1}{M}\sum_{i=1}^M \frac{U_{ii}+\hat{b}_i^2}{\hat{\mu}_i}
%\end{eqnarray}
%\end{frame}
%
%
%%=================================================================
%\begin{frame}{Practical Considerations with \texttt{RooUnfold}}
%  \texttt{RooUnfold} calculates the full covariance matrix $U_{ij}$. However, as far as I know, it does not calculate bias $\hat{b}_i$.
%  
%  Three options in my mind.
%  \begin{enumerate}
%    \item Handwave the expectation value calculation by assuming that the statistical uncertainties are negligible, i.e., $b_i=E[\hat{\mu}_i]-\mu_i \approx \hat{\mu}_i-\mu_i \approx \hat{b}_i$.
%  \end{enumerate}
%  If we really want to beat the problem to death, we can:
%  \begin{enumerate}
%    \setcounter{enumi}{1}
%    \item Generate toy MC spectra by varying counts in each bin of the true spectrum by Poisson (not reco, since reco undergoes smearing), smear them with the response matrix, do unfolding one by one, and calculate the bias.\\ \textcolor{red}{This is computationally infeasible.}
%    \item Calculate the bias faithfully. See the next slides.
%  \end{enumerate}
%\end{frame}
%
%
%%=================================================================
%\begin{frame}{Covariance Matrix Calculation in \texttt{RooUnfoldBayes}}
%From Tim's document, \href{https://arxiv.org/pdf/1105.1160.pdf}{Unfolding algorithms and tests using RooUnfold}, Eq's (2), (3), and (4) (sorry, now Tim's notation!):\\
%\footnotesize
%\textcolor{blue}{unfolding matrix:}
%\begin{equation}
%\hat{n}(C_i)=\sum_{j=1}^{n_E}M_{ij}n(E_j)\text{ where }M_{ij}=\frac{P(E_j|C_i)n_0(C_i)}{\epsilon_i\sum_{l=1}^{n_C}P(E_j|C_l)n_0(C_l)}
%\end{equation}
%\textcolor{blue}{error propagation matrix:}
%\begin{equation}\label{err_mat}
%\frac{\partial \hat{n}(C_i)}{\partial n(E_j)}=M_{ij}+\sum_{k=1}^{n_E}M_{ik}n(E_k)\left( \frac{1}{n_0(C_i)}\frac{\partial n_0(C_i)}{\partial n(E_j)}-\sum_{l=1}^{n_C}\frac{\epsilon_l}{n_0(C_l)}\frac{\partial n_0(C_l)}{\partial n(E_j)}M_{lk} \right)
%\end{equation}
%\textcolor{blue}{covariance matrix on the unfolded distribution:}
%\begin{equation}
%V(\hat{n}(C_k),\hat{n}(C_l))=\sum_{i,j=1}^{n_E}\frac{\partial \hat{n}(C_k)}{\partial n(E_i)}V(n(E_i),n(E_j))\frac{\partial \hat{n}(C_l)}{\partial n(E_j)}
%\end{equation}
%
%\end{frame}
%
%
%%=================================================================
%\begin{frame}{Bias Calculation with Iterative Unfolding}
%Back to Cowan's \href{http://www.ippp.dur.ac.uk/Workshops/02/statistics/proceedings/cowan.ps}{document} and notation, a generic formula for bias is
%\begin{equation}
%\hat{b}_i=\sum_{j=1}^{N}\frac{\partial \hat{\mu}_i}{\partial n_j}\left(\sum_{l=1}^M R_{jl}\hat{\mu}_l-n_j\right)
%\end{equation}
%, where $\frac{\partial \hat{\mu}_i}{\partial n_j}$ is the error propagation matrix given by Eq.~(\ref{err_mat}), $R$ is the response matrix, $\pmb{\hat{\mu}}$ is the unfolded spectrum, and $\pmb{n}$ is the measured reco spectrum.
%\end{frame}
%
%
%%=================================================================
%\begin{frame}{Going Forward with This Metric}
%Given Connor's variance and bias studies, if we go with this metric, we can expect some minimum before 5 iterations.\\
%If we can explain away with option 1, we will close this task in no time.
%\end{frame}


\end{document}